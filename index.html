
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }
  @media only screen and (max-width: 600px) {
    body {
      width: 90%;
    }
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>


<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/png" href="https://dibyaghosh.com/images/research/dnc.png">
    <title>Divide-and-Conquer Reinforcement Learning</title>
    <meta property='og:title' content='Divide-and-Conquer Reinforcement Learning' />
    <meta property="og:description" content="Ghosh, Singh, Rajeswaran, Kumar, Levine. Divide-and-Conquer Reinforcement Learning. In ICLR, 2018." />
    <meta property='og:url' content='https://dibyaghosh.com/dnc' />
    <meta property='og:video' content='dnc_video.mp4' />
    <meta property='og:image' content='https://dibyaghosh.com/images/research/dnc.png' />
  </head>

  <body>
        <br>
        <center><span style="font-size:4em;font-weight:bold;">Divide-and-Conquer  </br> Reinforcement Learning</span></center>

        <table align=center width=1000px>
          <tr>
            <td><center><span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~avisingh/" target="_blank">Dibya Ghosh<sup>1</sup></a></span></center></td>
            <td align=center width=200px><center><span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~avisingh/iccv17/" target="_blank">Avi Singh<sup>1</sup></a></span></center></td>
            <td align=center width=200px><center><span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~avisingh/iccv17/" target="_blank">Aravind Rajeswaran<sup>2</sup></a></span></center></td>
            <td align=center width=200px><center><span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~avisingh/iccv17/" target="_blank">Vikash Kumar<sup>2</sup></a></span></center></td>
            <td><center><span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine<sup>1</sup></a></span></center></td>
          <tr/>
         </table>
        <table align=center width=700px>
          <tr>
            <td align=center width=400px><center><span style="font-size:24px"><sup>1</sup>University of California, Berkeley</span></center></td>
            <td align=center width=300px><center><span style="font-size:24px"><sup>2</sup>University of Washington</span></center></td>

            <tr/>
        </table>
        <table align=center width=400px>
          <tr>
            <td align=center width=150px>
            <center><span style="font-size:24px"><a href="https://iclr.cc/Conferences/2018" target="_blank">ICLR 2018</a></span></center></td>
          <tr/>
        </table>
        <table align=center width=500px>
          <tr>
            <td align=center width=200px><center><span style="font-size:24px"><a href="https://arxiv.org/abs/1711.09874">[Paper]</a> <a href="https://github.com/dibyaghosh/dnc">[Code]</a></span></center></td>
        <tr/>
        </table><br/>

        <center id="demoVideo"><h1>Demo Video</h1></center>
          <table align=center width=600px>
            <tr><td align=center width=600px>
                <video width="1000" height="600" controls>
                    <source src="dnc_video.mp4" type="video/mp4">
                    <source src="dnc_video.ogv" type="video/ogg">
                    Your browser does not support the video tag.
                  </video>
            </td></tr>
          </table>

        <br>

        Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial,
         allowing them to optimize policies that can perform well even in highly stochastic environments. 
         However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, 
         making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead partitions the initial state space into
          "slices", and optimizes an ensemble of policies, each on a different slice. The ensemble is gradually unified into a single policy that can succeed on the whole state space. 
          This approach, which we term <i>divide-and-conquer RL</i>, is able to solve complex tasks where conventional deep RL methods are ineffective. 
          Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, 
          and locomotion tasks, and exceeds the performance of a variety of prior methods.
        
      <br>
      <br>
      <hr>

      <br>
      <hr>

        <table align=center width=1000px>
          <tr><td width=1000px>
            <center><a href="teaser.png"><img src = "teaser.png" height="400px"></img></a><br></center>
          </td></tr>
        </table>

        <center id="sourceCode"><h1>Our Solution</h1></center>
        For problems with high initial state variability, we partition the initial state space of the task into contexts using clustering algorithms. We train an ensemble of <i> local </i> policies, each specialized to a particular context. The objective for each policy is to maximize expected reward and minimize the KL divergence from each of the other policies in the ensemble. On a slower timescale, the ensemble is distilled together into a <i> global </i> policy, which can operate on
        the original task. 
        <br/>

      <br>
      <hr>

        <table align=center width=850px>
          <center><h1>Paper</h1></center>
          <tr>
          <td width=400px align=left>
          <a href="https://arxiv.org/pdf/1711.09874.pdf"><img style="height:200px" src="preview.jpg"/></a>
          <center>
          <span style="font-size:20pt"><a href="https://arxiv.org/pdf/1711.09874.pdf">[Paper PDF]</a>&nbsp;
          <span style="font-size:20pt"><a href="https://arxiv.org/abs/1711.09874">[arXiv]</a>
          </center>
          </td>
          <td width=50px align=center>
          </td>
          <td width=450px align=left>
          <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, Sergey Levine <br/> <b>Divide-and-Conquer Reinforcement Learning</b> In <i>ICLR 2018</i>.</span></p>
          <span style="font-size:20pt"><a href="resources/ref.bib">[Bibtex]</a>
          </td>
          </tr>
          <tr>
          <td width=350px align=left>
          </td>
          <td width=100px align=center>
          </td>
          <td width=450px align=left>
            </td>
            </tr>
        </table>


    <br><br>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

</body>
</html>
